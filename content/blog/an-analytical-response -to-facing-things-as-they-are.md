+++
date = "2017-08-27T10:17:00Z"
title = "An analytical response to ‘Facing things as they are'"
draft = false
categories = ["analysis"]
description = ""
featuredpath = "date"
linktitle = ""
type = "post"

+++

[Will Myddleton](https://twitter.com/myddelton) recently posted about [Facing things as they are](http://www.myddelton.co.uk/blog/facing-things-as-they-are) — as a fundamental of user research and an approach to life. As an analyst and as a meditator who works (sometimes successfully) to appreciate the moment, I really valued what Will has written. I said I'd try to respond from an analyst's perspective.

As analysts working in multi-disciplinary teams, we have a number of tasks:

1. Gather and analyse existing evidence
2. Identify a testable hypothesis
3. Make sure the data we need is collected and is appropriate
4. Do the analysis
5. Tell an actionable story

So this is a very parallel, indeed intermingled, journey to that described by Will.

> User research is about starting by facing things as they are.

And so is analysis. It's why it's so important to gather existing evidence about the issue you're working on, using, for example, digital analytics from legacy products, call centre data, text analysis, search analytics. With this data, you can build up a picture of the current landscape.

When the quantitative data and the user research data are looked at together, we can get a richer understanding of 'how things are'. Can we see findings from user research replicated at scale in the analytics? Can some behaviour that we see in the analytics be explained by the user research?

Then the team will be in a better place to determine what to work on, and importantly, to develop a hypothesis about how that work will make things better.

So the work gets done. But did it have the intended effect?

Again we need to face things — do the user research and analytics data tell us that the work we've done has made the impact it intended?

Will continues:

> Facing things as they are often leads to a dawning realisation that the things we are working on are the wrong things.

Yes the data may tell us that; or a less dramatic, 'the thing we worked on didn't have the impact we expected'. Of course, we may have had a positive impact.

But again we're challenged to face things as they are — to use the evidence we have and the learning we've made to re-prioritise, change direction or build on our success.
